# Introduction

I enrolled in *“Apache Airflow: The Hands-On Guide”*, an A-to-Z course taught by **Marc Lamberti**, **Head of Customer Education at Astronomer**, and I mastered how to programmatically author, schedule, and monitor workflows with Airflow’s TaskFlow API, operators, executors, and integrations.

## Throughout the course, I:
- Learned Airflow fundamentals — how the scheduler, web server, and metadata database interact.
- Built the Forex Data Pipeline, exploring HTTP, Spark, Hadoop, Slack integrations, and more.
- Mastered DAG design: timezones, catchup, unit testing, folder structure, subDAGs, cross-DAG dependencies, and deadlock avoidance.
- Scaled Airflow with Local, Celery, and Kubernetes Executors; specialized workers; handled node failures; and ran a 3-node Rancher cluster locally.
- Templated DAGs, implemented DAG dependencies, and deep-dived into advanced concepts like SubDAGs and deadlocks.
- Deployed an AWS EKS cluster with Rancher for cloud-based KubernetesExecutor runs.
- Monitored Airflow using Elasticsearch and Grafana dashboards.
- Secured my instance with RBAC, authentication, password policies, and data encryption.

By completing hands-on exercises and quizzes at each section, I reinforced best practices and emerged confident in using Airflow in production environments.

## Getting Started

1. **Clone the repo**
   ```bash
   git clone https://github.com/BrunoChiconato/airflow-project.git
   cd airflow-project
   ```